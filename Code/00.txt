# R script to prepare data and lagged variables for INLA-DLNM modelling

# install INLA
# install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE)

# load INLA
library(INLA)

#  select other packages
packages <- c("data.table", "tidyverse", "sf", "sp", "spdep",
              "dlnm", "tsModel", "Metrics","RColorBrewer", 
              "geofacet", "ggpubr", "ggthemes", "caret")

# install.packages
# lapply(packages, install.packages, character.only = TRUE)

# load packages
lapply(packages, library, character.only = TRUE)

# load shape file for Brazil
map <- read_sf("data/NUTS_RG_20M_2021_3035.shp")
# dim(map)

# Create adjacency matrix
nb.map <- poly2nb(as_Spatial(map$geometry))
g.file <- "output/map.graph"
if (!file.exists(g.file)) nb2INLA(g.file, nb.map)

# load pre-defined grid of Brazilian states for geofacet plots
# note: could use pre-loaded grid = "br_states_grid1" in geofacet package, would need match state names
grid <- read.csv("data/br_states_grid.csv")
# head(grid)


# load data
# note dengue data available from Jan 2001 
# Climate data included for 2000 to obtained lagged values prior to 2001
data <- fread("data/data_2000_2019.csv", header = T)
# head(data)

# summary climate variables
summary(data$tmin)
summary(data$tmax)
summary(data$pdsi)


# set maximum lag
nlag = 6  # Number of lags

# No2
lag_NO2 <- tsModel::Lag(data$NO2, group = data$year, k = 0:nlag)
#PM10
lag_PM10 <- tsModel::Lag(data$PM10, group = data$year, k = 0:nlag)
#PM25
lag_PM25 <- tsModel::Lag(data$PM2.5, group = data$year, k = 0:nlag)
#O3
lag_O3 <- tsModel::Lag(data$O3, group = data$year, k = 0:nlag)
# Minimum temperature (Tmin)
lag_tmin <- tsModel::Lag(data$tmin, group = data$micro_code, k = 0:nlag)
# Maximum temperature (Tmax)
lag_tmax <- tsModel::Lag(data$tmax, group = data$micro_code, k = 0:nlag)
#Mean Temperature
lag_Temperature <- tsModel::Lag(data$Temperature, group = data$year, k = 0:nlag)


# Remove year 2020 from lagged climate variables

lag_NO2_filtered <- subset(lag_NO2, year != 2020)
lag_PM10_filtered <- subset(lag_PM10, year != 2020)
lag_PM25_filtered <- subset(lag_PM25, year != 2020)
lag_O3_filtered <- subset(lag_O3, year != 2020)
lag_Temperature_filtered <- subset(lag_Temperature, year != 2020)

# define dimensions
# total number of years
nyear <- length(unique(data$year))

# total number of weeks
nweek <- length(unique(data$week))

# total number of NUTS3 regions (assuming NUTS3 is a region identifier)
nnuts3 <- length(unique(data$NUTS3))

# define cross-basis matrix (combining nonlinear exposure and lag functions)
# set lag knots
lagknot = equalknots(0:nlag, 2)

basis_NO2 <- crossbasis(lag_NO2, argvar = list(fun = "ns", knots = equalknots(data_filtered$NO2, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_PM10 <- crossbasis(lag_PM10, argvar = list(fun = "ns", knots = equalknots(data_filtered$PM10, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_PM25 <- crossbasis(lag_PM25, argvar = list(fun = "ns", knots = equalknots(data_filtered$PM2.5, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_O3 <- crossbasis(lag_O3, argvar = list(fun = "ns", knots = equalknots(data_filtered$O3, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_Temperature <- crossbasis(lag_Temperature, argvar = list(fun = "ns", knots = equalknots(data_filtered$Temperature, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_MaxTm <- crossbasis(lag_MaxTm, argvar = list(fun = "ns", knots = equalknots(data_filtered$MaxTm, 2)), arglag = list(fun = "ns", knots = nlag/2))
basis_MinTm <- crossbasis(lag_MinTm, argvar = list(fun = "ns", knots = equalknots(data_filtered$MinTm, 2)), arglag = list(fun = "ns", knots = nlag/2))


# test linear interaction with % of land use by province 
summary(data$land_use)
# Set indicator to zero at the point of interest (centering point)
# Re-parameterize model to extract different predictions
land_use_ind1 <- data$land_use - quantile(data$land_use, p = 0.75) # Highly urbanized
land_use_ind2 <- data$land_use - quantile(data$land_use, p = 0.5)  # Intermediate
land_use_ind3 <- data$land_use - quantile(data$land_use, p = 0.25) # More rural

# test linear interaction with density of population by province 
summary(data$pop_density)
# Set indicator to zero at the point of interest (centering point)
# Re-parameterize model to extract different predictions
pop_density_ind1 <- data$pop_density - quantile(data$pop_density, p = 0.75) # High density
pop_density_ind2 <- data$pop_density - quantile(data$pop_density, p = 0.5)  # Intermediate density
pop_density_ind3 <- data$pop_density - quantile(data$pop_density, p = 0.25) # Low density

# Interaction terms between % of land use and  density of population by province 
interaction1 <- land_use_ind1 * pop_density_ind1
interaction2 <- land_use_ind2 * pop_density_ind2
interaction3 <- land_use_ind3 * pop_density_ind3


# Multiply each cross-basis variable by the linear terms (see Gasparrini et al. EHP 2015)
# note: exploit the column by column product 

# Highly urbanized areas
basis_NO2_land_use1 <- basis_NO2 * land_use_ind1
basis_PM10_land_use1 <- basis_PM10 * land_use_ind1
basis_PM25_land_use1 <- basis_PM25 * land_use_ind1
basis_O3_land_use1 <- basis_O3 * land_use_ind1
basis_MaxTm_land_use1 <- basis_MaxTm * land_use_ind1
basis_MinTm_land_use1 <- basis_MinTm * land_use_ind1

# Intermediate urbanization
basis_NO2_land_use2 <- basis_NO2 * land_use_ind2
basis_PM10_land_use2 <- basis_PM10 * land_use_ind2
basis_PM25_land_use2 <- basis_PM25 * land_use_ind2
basis_O3_land_use2 <- basis_O3 * land_use_ind2
basis_MaxTm_land_use2 <- basis_MaxTm * land_use_ind2
basis_MinTm_land_use2 <- basis_MinTm * land_use_ind2

# More rural areas
basis_NO2_land_use3 <- basis_NO2 * land_use_ind3
basis_PM10_land_use3 <- basis_PM10 * land_use_ind3
basis_PM25_land_use3 <- basis_PM25 * land_use_ind3
basis_O3_land_use3 <- basis_O3 * land_use_ind3
basis_MaxTm_land_use3 <- basis_MaxTm * land_use_ind3
basis_MinTm_land_use3 <- basis_MinTm * land_use_ind3


# High density
basis_NO2_pop_density1 <- basis_NO2 * pop_density_ind1
basis_PM10_pop_density1 <- basis_PM10 * pop_density_ind1
basis_PM25_pop_density1 <- basis_PM25 * pop_density_ind1
basis_O3_pop_density1 <- basis_O3 * pop_density_ind1
basis_MaxTm_pop_density1 <- basis_MaxTm * pop_density_ind1
basis_MinTm_pop_density1 <- basis_MinTm * pop_density_ind1

# Intermediate density
basis_NO2_pop_density2 <- basis_NO2 * pop_density_ind2
basis_PM10_pop_density2 <- basis_PM10 * pop_density_ind2
basis_PM25_pop_density2 <- basis_PM25 * pop_density_ind2
basis_O3_pop_density2 <- basis_O3 * pop_density_ind2
basis_MaxTm_pop_density2 <- basis_MaxTm * pop_density_ind2
basis_MinTm_pop_density2 <- basis_MinTm * pop_density_ind2

# Low density
basis_NO2_pop_density3 <- basis_NO2 * pop_density_ind3
basis_PM10_pop_density3 <- basis_PM10 * pop_density_ind3
basis_PM25_pop_density3 <- basis_PM25 * pop_density_ind3
basis_O3_pop_density3 <- basis_O3 * pop_density_ind3
basis_MaxTm_pop_density3 <- basis_MaxTm * pop_density_ind3
basis_MinTm_pop_density3 <- basis_MinTm * pop_density_ind3

#Iteraction effects 
# Multiply cross-basis variables by interaction terms
# Interaction with first level (highly urbanized and high density)
basis_NO2_interaction1 <- basis_NO2 * interaction1
basis_PM10_interaction1 <- basis_PM10 * interaction1
basis_PM25_interaction1 <- basis_PM25 * interaction1
basis_O3_interaction1 <- basis_O3 * interaction1
basis_MaxTm_interaction1 <- basis_MaxTm * interaction1
basis_MinTm_interaction1 <- basis_MinTm * interaction1

# Interaction with second level (intermediate urbanization and intermediate density)
basis_NO2_interaction2 <- basis_NO2 * interaction2
basis_PM10_interaction2 <- basis_PM10 * interaction2
basis_PM25_interaction2 <- basis_PM25 * interaction2
basis_O3_interaction2 <- basis_O3 * interaction2
basis_MaxTm_interaction2 <- basis_MaxTm * interaction2
basis_MinTm_interaction2 <- basis_MinTm * interaction2

# Interaction with third level (more rural and low density)
basis_NO2_interaction3 <- basis_NO2 * interaction3
basis_PM10_interaction3 <- basis_PM10 * interaction3
basis_PM25_interaction3 <- basis_PM25 * interaction3
basis_O3_interaction3 <- basis_O3 * interaction3
basis_MaxTm_interaction3 <- basis_MaxTm * interaction3
basis_MinTm_interaction3 <- basis_MinTm * interaction3


#Create an Index for NUTS3 Regions (assuming NUTS3 is a region identifier similar to microregions)
# Assuming the number of unique NUTS3 regions is previously calculated as nnuts3
data$nuts3_index <- rep(1:nnuts3, each = ntime)


Create a autonomy Index (NUTS2)

# Assuming the number of unique states is previously calculated as nstate
#k <- unique(data$state_code)
#data$auto_index <- rep(NA, nrow(data))  # Initialize the column with NA

#for (j in 1:nstate) {
#  data$state_index[data$state_code == k[j]] <- j
#}


#Create a Year Index:
#Assuming the earliest year in your dataset is 2017 and you want to set it to 1:

data$year_index <- data$year - 2016  # Subtract 2016 to make 2017 as 1

# set up data and priors for INLA model

# set data for models
Y  <- data$mort # response variable
N  <- length(Y) # total number of data points
E  <- data$popu/10^5 # model offset so that response is equivalent to an incidence rate per 100,000 people
T1 <- data$week # for random effect to account for annual cycle (seasonality)
T2 <- data$year_index # for random effect to account for inter-annual variability
S1 <- data$nuts3_index_index # for microregion spatial random effect
S2 <- data$auto_index # for state interaction with month random effect
Vu <- data$land_use # include level of urbanisation (% pop living in urban areas) variable along with linear urban interaction
Vw <- data$pop_density # include frequency of water shortages along with linear water shortage interaction


# create dataframe for model testing
df <- data.frame(Y, E, T1, T2, S1, S2, Vu, Vw)

# define priors
precision.prior <- list(prec = list(prior = "pc.prec", param = c(0.5, 0.01)))

# inla model function

# include formula and set defaults for data, family (to allow other prob dist models e.g. Poisson) and config (to allow for sampling)
mymodel <- function(formula, data = df, family = "nbinomial", config = FALSE)

  {
  model <- inla(formula = formula, data = data, family = family, offset = log(E),
       control.inla = list(strategy = 'adaptive'), 
       control.compute = list(dic = TRUE, config = config, 
                              cpo = TRUE, return.marginals = FALSE),
       control.fixed = list(correlation.matrix = TRUE, 
                            prec.intercept = 1, prec = 1),
       control.predictor = list(link = 1, compute = TRUE), 
       verbose = FALSE)
  model <- inla.rerun(model)
  return(model)
}
